{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you begin\n",
        "\n",
        "Please <font color='red'>**MAKE A COPY**</font> of this colab to make sure your progress is saved.\n",
        "\n"
      ],
      "metadata": {
        "id": "TTJrI8nM8ycY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical ML with JAX\n",
        "\n",
        "In this tutorial, we'll see how to use JAX to train a neural network."
      ],
      "metadata": {
        "id": "oX84nB6_8_tk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOMrewRLIuAk"
      },
      "outputs": [],
      "source": [
        "!pip install jax numpy pandas matplotlib optax flax -U -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Is you get a \"std::bad_cast\" error, click on \"Runtime > Restart session and run all\"."
      ],
      "metadata": {
        "id": "RWXL1wZwPESH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import numpy as np\n",
        "import optax\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from pprint import pprint\n",
        "from jax import tree_util"
      ],
      "metadata": {
        "id": "9KVq-a6LOd8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic JAX"
      ],
      "metadata": {
        "id": "6FYQUwBCMAj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        " return x**2 + 5\n",
        "\n",
        "f(2.0) # Note: 2² + 5 = 9"
      ],
      "metadata": {
        "id": "Lk_fp1weOdR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(jnp.array([6.0, 9.0]))"
      ],
      "metadata": {
        "id": "g64Hi1XLPa8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = jax.grad(f)\n",
        "df(2.0)\n",
        "# Note: d(x² + 5)/dx = 2x = 4"
      ],
      "metadata": {
        "id": "uvdCim1jPbSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent"
      ],
      "metadata": {
        "id": "fOfGi6CcMDwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to find x that minimizes \"loss_fn\".\n",
        "def loss_fn(x):\n",
        "  return x**2\n",
        "\n",
        "\n",
        "# Initial guess for x\n",
        "x = jnp.array(2.0)\n",
        "print(\"Initial x: \", x)\n",
        "\n",
        "# Gradient of loss_fn according to x\n",
        "gradient_fn = jax.grad(loss_fn)\n",
        "\n",
        "# Gradient descent algorithm\n",
        "learning_rate = 0.2\n",
        "num_iterations = 10\n",
        "for _ in range(num_iterations):\n",
        "  loss = loss_fn(x)\n",
        "  gradient = gradient_fn(x)\n",
        "  x = x - learning_rate * gradient\n",
        "  print(f\"x:{x:.5f} loss:{loss:.5f} gradient:{gradient:.5f}\")\n",
        "\n",
        "print(\"Final x: \", x)"
      ],
      "metadata": {
        "id": "rU3IIYNzPTrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent with other algorithms\n",
        "\n",
        "The classical gradient descent algorithm is slow to converge when there are many dimensions (not really the case use). When using gradient descent on neural networks, it is better to use Adam, AdaGrad, RMSProp, or Momentum optimizers ([see details](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)).\n",
        "\n",
        "Optax has a large collection of already implemented optimizers. Here is an example with Adam.\n",
        "\n"
      ],
      "metadata": {
        "id": "cS24EAAPSSMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial guess for x\n",
        "x = jnp.array(2.0)\n",
        "print(\"Initial x: \", x)\n",
        "\n",
        "# Gradient descent algorithm with Adam\n",
        "optimizer = optax.adamw(learning_rate)\n",
        "optimizer_state = optimizer.init(x)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  loss = loss_fn(x)\n",
        "  gradient = gradient_fn(x)\n",
        "\n",
        "  updates, optimizer_state = optimizer.update(gradient, optimizer_state, params=x)\n",
        "  x = optax.apply_updates(x, updates)\n",
        "\n",
        "  print(f\"x:{x:.5f} loss:{loss:.5f} gradient:{gradient:.5f}\")\n",
        "\n",
        "print(\"Final x: \", x)"
      ],
      "metadata": {
        "id": "Udl79BmKSoQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple MLP"
      ],
      "metadata": {
        "id": "ssDBb1mOMIJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(params, x):  # The model prediction\n",
        "  x = jax.nn.relu(jnp.dot(x, params['w1']))\n",
        "  x = jnp.dot(x, params['w2'])\n",
        "  return x\n",
        "\n",
        "\n",
        "def get_batches(batches=100, batch_size=200, dims=10):  # Gen some synthetic data\n",
        "  for i in range(batches):\n",
        "    x = np.random.randn(batch_size, dims)\n",
        "    y = (np.sum(x, axis=1) > 0).astype(int)\n",
        "    yield x, y\n",
        "\n",
        "\n",
        "def loss_fn(params, x, y):  # Objective\n",
        "  logits = mlp(params, x)\n",
        "  return optax.sigmoid_binary_cross_entropy(logits, y).mean()\n",
        "\n",
        "\n",
        "# Function value and its derivative together.\n",
        "loss_and_grads_fn = jax.value_and_grad(loss_fn)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, x, y, lr=0.1):\n",
        "  loss, grads = loss_and_grads_fn(params, x, y)\n",
        "  new_params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n",
        "  return new_params, loss\n",
        "\n",
        "\n",
        "# Initial model parameter values\n",
        "key = jax.random.PRNGKey(1)\n",
        "key, key_w1, key_w2 = jax.random.split(key, 3)\n",
        "params = {\n",
        "    'w1': jax.random.normal(key_w1, (10, 10)),\n",
        "    'w2': jax.random.normal(key_w2, (10)),\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "  for batch_x, batch_y in get_batches():\n",
        "    params, loss = train_step(params, batch_x, batch_y)\n",
        "  print(f'Epoch: {epoch} Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "FiCIxtV8Pqnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionaries and trees\n",
        "\n",
        "In JAX, parameters are stored as dictionaries of JAX arrays. Many functions works on and expect those.\n",
        "\n",
        "Learn more with the [Working with pytrees](https://jax.readthedocs.io/en/latest/working-with-pytrees.html) tutorial."
      ],
      "metadata": {
        "id": "Q9KtC6xmQyHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = {\n",
        "    \"a\": jnp.array(1.),\n",
        "    \"b\": jnp.array([1., 2., 3.]),\n",
        "    \"c\": {\n",
        "        \"d\": jnp.array(1.),\n",
        "        \"e\": jnp.array(2.),\n",
        "    },\n",
        "}\n",
        "pprint(t)"
      ],
      "metadata": {
        "id": "VVpUFfQ2RCsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 1 to all the values\n",
        "pprint(jax.tree.map(lambda x:x+1, t))"
      ],
      "metadata": {
        "id": "LsySPVOTRmtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradient\n",
        "def f(v):\n",
        "  return (v[\"a\"] * v[\"b\"] * ( v[\"c\"][\"d\"] + v[\"c\"][\"e\"] )).mean()\n",
        "f(t)"
      ],
      "metadata": {
        "id": "H5Ymtf3ER0cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.grad(f)(t)"
      ],
      "metadata": {
        "id": "9c3rb3jeSAoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jitting\n",
        "\n",
        "Instead of executing operations one after the other in python, \"jitting\" a function merges all the instructions and compiles them (the same way you would compile a program in c++).\n",
        "\n",
        "Compilation takes a little bit of time, and is less flexible than execution in python, but it is much faster (especially when using hardware accelerators).\n",
        "\n",
        "A jitted python function is only executed once. The other calls will call the compiled function. A `print` in the function will only be called once. To print something each time the function is executed (great for debug), use `jax.debug.print` instead.\n",
        "\n",
        "Jitting / compilation is typed. if you call your function with arguments having other types, the function will be re-compiled.\n",
        "\n",
        "Learn more [here](https://jax.readthedocs.io/en/latest/jit-compilation.html)."
      ],
      "metadata": {
        "id": "RtmaFksRT4Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f(x):\n",
        "  print(\"Compiling function for\", x.dtype)\n",
        "  jax.debug.print(\"Running with x={x}\", x=x)\n",
        "  return 2 * x + x\n",
        "\n",
        "\n",
        "# Calling function with a 32 bits integer input.\n",
        "f(1)\n",
        "f(2)\n",
        "f(3)\n",
        "\n",
        "# Calling function with a 32 bits float input.\n",
        "# The function will be re-compiled.\n",
        "f(1.0)\n",
        "f(2.0)\n",
        "f(3.0)"
      ],
      "metadata": {
        "id": "pxfzi0eYhsDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data placement"
      ],
      "metadata": {
        "id": "jr3pXC1RkpZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Devices available on your machine.\n",
        "# Note: By default, Google Colab only have a CPU. In the settings, you can request a TPU for free (if some are available).\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "8dZHVZrqkq4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, all the computation is done on GPU / TPU is one is available.\n",
        "This is why JAX is something slower than numpy on some operations.\n",
        "Otherwise, computation is done on CPU.\n",
        "\n"
      ],
      "metadata": {
        "id": "U5KIzv4Al35_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = jnp.array([1,2,3])\n",
        "print(\"a is currently stored on\", a.device)"
      ],
      "metadata": {
        "id": "tI4GvYU7l6fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = jax.device_put(a, jax.devices()[-1])\n",
        "print(\"a is now stored on\", a.device)"
      ],
      "metadata": {
        "id": "e3s8Kd3omP9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JAX and Numpy\n",
        "\n",
        "`jax.numpy` (generally alised as `jnp`) has the same API as numpy.\n",
        "\n",
        "A `jnp.array` is a JAX array, which is different from a `numpy.array`.\n",
        "\n",
        "It is possible to convert arrays JAX <-> Numpy.\n",
        "\n",
        "For options with a lot of computation (e.g., multiplying matrices), JAX arrays are generally faster. For operations with a lot of data transfer (e.g., preparing a dataset), Numpy arrays are generally faster."
      ],
      "metadata": {
        "id": "TXSj03XTjXfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A numpy array\n",
        "a = np.array([1,2,3])\n",
        "type(a)"
      ],
      "metadata": {
        "id": "Z-fmQthjkCF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A JAX array\n",
        "b = jnp.array([1,2,3])\n",
        "type(b)"
      ],
      "metadata": {
        "id": "IebBziHwkGkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting a numpy array into a jax array\n",
        "type(jnp.asarray(a))"
      ],
      "metadata": {
        "id": "qIYYE5KukKUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting a jax array into a numpy array\n",
        "type(np.array(b))"
      ],
      "metadata": {
        "id": "SE49THfOkPTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLAX's module can help tracking weights\n",
        "\n",
        "In JAX, weights are always stored in a dictionary (see above).\n",
        "FLAX's module can help creating and managing those weights."
      ],
      "metadata": {
        "id": "bmGv8azsFPB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "  output_dim: int = 2\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x) -> jax.Array:\n",
        "    # Create a layer. Also create two weight matrices of size\n",
        "    # [input_dim, output_dim] and [output_dim]\n",
        "    layer_1 = nn.Dense(features=self.output_dim, name=\"my_layer_1\")\n",
        "\n",
        "    # Create another layer\n",
        "    layer_2 = nn.Dense(features=self.output_dim, name=\"my_layer_2\")\n",
        "\n",
        "    # Apply the layer on the data\n",
        "    jax.debug.print(\"Running with x={x}\", x=x)\n",
        "    x = layer_2(layer_1(x))\n",
        "    return x\n",
        "\n",
        "# Initialize the model\n",
        "model = MyModel()\n",
        "x = jnp.array([1., 2., 3.])\n",
        "# Create the dictionary of weights.\n",
        "model_state = model.init(jax.random.PRNGKey(0), x)\n",
        "\n",
        "model_state"
      ],
      "metadata": {
        "id": "HjR8lDdQFSwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can call the model with the weights."
      ],
      "metadata": {
        "id": "1xdDkxTrGrq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.apply(model_state, x)"
      ],
      "metadata": {
        "id": "8SnmynsiGvUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together to train a MLP"
      ],
      "metadata": {
        "id": "jtsusssUHPiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our model\n",
        "class MLP(nn.Module):\n",
        "  num_layers: int = 2\n",
        "  layer_size: int = 10\n",
        "  output_dim: int = 1\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x) -> jax.Array:\n",
        "    for i in range(self.num_layers):\n",
        "      x = nn.Dense(features=self.layer_size, name=f'layer_{i}')(x)\n",
        "      x = nn.relu(x)\n",
        "    x = nn.Dense(features=self.output_dim, name='final_layer')(x)\n",
        "    x = jnp.squeeze(x, axis=1)\n",
        "    return x\n",
        "\n",
        "# Initialize model and its weights\n",
        "model = MLP()\n",
        "key, model_init_key = jax.random.split(key, 2)\n",
        "x_sample, _ = next(iter(get_batches()))\n",
        "model_state = model.init(model_init_key, x_sample)[\"params\"]\n",
        "\n",
        "# Print the weights of the model\n",
        "print(\"model_state:\\n\", jax.tree.map(lambda x:x.shape,model_state))\n",
        "\n",
        "# Print the internal layers of the model, their side and number of flops.\n",
        "print(\"Model structure:\\n\", model.tabulate(\n",
        "          model_init_key,\n",
        "          x_sample,\n",
        "          compute_flops=True,\n",
        "          compute_vjp_flops=True,\n",
        "      ))\n",
        "\n",
        "# Initialize the optimizer (we use AdamW)\n",
        "optimizer = optax.adamw(0.1)\n",
        "optimizer_state = optimizer.init(model_state)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(model_state, optimizer_state, x, y):\n",
        "\n",
        "  def loss_fn(model_state, x, y):\n",
        "    logits = model.apply({\"params\": model_state}, x)\n",
        "    return optax.sigmoid_binary_cross_entropy(logits, y).mean()\n",
        "\n",
        "  # Gradient descent\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(model_state, x, y)\n",
        "  updates, new_optimizer_state = optimizer.update(grads, optimizer_state, params=model_state)\n",
        "  new_model_state = optax.apply_updates(model_state, updates)\n",
        "\n",
        "  return new_model_state, new_optimizer_state, loss\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "  for batch_x, batch_y in get_batches():\n",
        "    model_state, optimizer_state, loss = train_step(model_state, optimizer_state, batch_x, batch_y)\n",
        "  print(f'Epoch: {epoch} Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "cADH0DtGHpQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}