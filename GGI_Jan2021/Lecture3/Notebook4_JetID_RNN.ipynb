{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Jet Tagging with **Recurrent Neural Network** \n",
    "\n",
    "---\n",
    "In this notebook, we perform a Jet identification task using a multiclass classifier with a GRU unit.\n",
    "Gated Recurrent Units are one kind of RNNs. \n",
    "\n",
    "The problem consists in identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
    "based on a jet image, i.e., a 2D histogram of the transverse momentum ($p_T$) deposited in each of 100x100\n",
    "bins of a square window of the ($\\eta$, $\\phi$) plane, centered along the jet axis.\n",
    "\n",
    "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf \n",
    "\n",
    "For details on the dataset, see Notebook1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage \n",
    "\n",
    "To type shell commands, we start the command line with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tutorials'...\n",
      "remote: Enumerating objects: 67, done.\u001b[K\n",
      "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "^Cceiving objects:  13% (72/548), 35.02 MiB | 6.31 MiB/s\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/pierinim/tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jetImage_7_100p_0_10000.h5     jetImage_7_100p_50000_60000.h5\r\n",
      "jetImage_7_100p_10000_20000.h5 jetImage_7_100p_60000_70000.h5\r\n",
      "jetImage_7_100p_30000_40000.h5 jetImage_7_100p_70000_80000.h5\r\n",
      "jetImage_7_100p_40000_50000.h5 jetImage_7_100p_80000_90000.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls tutorials/Data/JetDataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending tutorials/Data/JetDataset/jetImage_7_100p_30000_40000.h5\n",
      "Appending tutorials/Data/JetDataset/jetImage_7_100p_60000_70000.h5\n",
      "Appending tutorials/Data/JetDataset/jetImage_7_100p_50000_60000.h5\n",
      "Appending tutorials/Data/JetDataset/jetImage_7_100p_10000_20000.h5\n",
      "Appending tutorials/Data/JetDataset/jetImage_7_100p_0_10000.h5\n",
      "(50000, 5) (50000, 100, 16)\n"
     ]
    }
   ],
   "source": [
    "target = np.array([])\n",
    "jetList = np.array([])\n",
    "# we cannot load all data on Colab. So we just take a few files\n",
    "datafiles = ['tutorials/Data/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "           'tutorials/Data/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "            'tutorials/Data/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "# if you are running locallt, you can use the full dataset doing\n",
    "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    myJetList = np.array(f.get(\"jetConstituentList\"))\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetList = np.concatenate([jetList, myJetList], axis=0) if jetList.size else myJetList\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "    del myJetList, mytarget\n",
    "    f.close()\n",
    "print(target.shape, jetList.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 50000 with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images.\n",
    "\n",
    "---\n",
    "\n",
    "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33500, 100, 16) (16500, 100, 16) (33500, 5) (16500, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(jetList, target, test_size=0.33)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "del jetList, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, GRU, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureArrayLength = (X_train.shape[1],X_train.shape[2])\n",
    "dropoutRate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 12:29:43.759280 4591664576 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "inputList = Input(shape=(featureArrayLength))\n",
    "x = GRU(40, activation=\"tanh\", recurrent_activation='sigmoid')(inputList)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "#\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "#\n",
    "x = Dense(10, activation='relu')(x)\n",
    "x = Dropout(dropoutRate)(x)\n",
    "x = Dense(5, activation='relu')(x)\n",
    "#\n",
    "output = Dense(5, activation='softmax')(x)\n",
    "####\n",
    "model = Model(inputs=inputList, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 16)]         0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 40)                6840      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 7,955\n",
      "Trainable params: 7,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33500 samples, validate on 16500 samples\n",
      "Epoch 1/200\n",
      "33500/33500 - 13s - loss: 1.6009 - val_loss: 1.5092\n",
      "Epoch 2/200\n",
      "33500/33500 - 12s - loss: 1.4797 - val_loss: 1.4385\n",
      "Epoch 3/200\n",
      "33500/33500 - 13s - loss: 1.4307 - val_loss: 1.4014\n",
      "Epoch 4/200\n",
      "33500/33500 - 12s - loss: 1.4055 - val_loss: 1.3736\n",
      "Epoch 5/200\n",
      "33500/33500 - 12s - loss: 1.3895 - val_loss: 1.3574\n",
      "Epoch 6/200\n",
      "33500/33500 - 16s - loss: 1.3661 - val_loss: 1.3521\n",
      "Epoch 7/200\n",
      "33500/33500 - 16s - loss: 1.3540 - val_loss: 1.3377\n",
      "Epoch 8/200\n",
      "33500/33500 - 14s - loss: 1.3482 - val_loss: 1.3279\n",
      "Epoch 9/200\n",
      "33500/33500 - 13s - loss: 1.3372 - val_loss: 1.3195\n",
      "Epoch 10/200\n",
      "33500/33500 - 14s - loss: 1.3329 - val_loss: 1.3386\n",
      "Epoch 11/200\n",
      "33500/33500 - 14s - loss: 1.3250 - val_loss: 1.3097\n",
      "Epoch 12/200\n",
      "33500/33500 - 15s - loss: 1.3194 - val_loss: 1.3169\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "33500/33500 - 13s - loss: 1.3150 - val_loss: 1.3192\n",
      "Epoch 14/200\n",
      "33500/33500 - 13s - loss: 1.3069 - val_loss: 1.2988\n",
      "Epoch 15/200\n",
      "33500/33500 - 12s - loss: 1.3036 - val_loss: 1.2983\n",
      "Epoch 16/200\n",
      "33500/33500 - 13s - loss: 1.3021 - val_loss: 1.2974\n",
      "Epoch 17/200\n",
      "33500/33500 - 12s - loss: 1.3007 - val_loss: 1.2983\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "33500/33500 - 13s - loss: 1.3014 - val_loss: 1.2986\n",
      "Epoch 19/200\n",
      "33500/33500 - 13s - loss: 1.2991 - val_loss: 1.2966\n",
      "Epoch 20/200\n",
      "33500/33500 - 13s - loss: 1.2994 - val_loss: 1.2964\n",
      "Epoch 21/200\n",
      "33500/33500 - 13s - loss: 1.3002 - val_loss: 1.2964\n",
      "Epoch 22/200\n",
      "33500/33500 - 14s - loss: 1.2986 - val_loss: 1.2963\n",
      "Epoch 23/200\n",
      "33500/33500 - 14s - loss: 1.2991 - val_loss: 1.2963\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "33500/33500 - 14s - loss: 1.3001 - val_loss: 1.2962\n",
      "Epoch 25/200\n",
      "33500/33500 - 13s - loss: 1.2995 - val_loss: 1.2962\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "33500/33500 - 13s - loss: 1.2990 - val_loss: 1.2962\n",
      "Epoch 27/200\n",
      "33500/33500 - 13s - loss: 1.3007 - val_loss: 1.2962\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "33500/33500 - 12s - loss: 1.2995 - val_loss: 1.2962\n",
      "Epoch 29/200\n",
      "33500/33500 - 13s - loss: 1.2993 - val_loss: 1.2962\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "33500/33500 - 13s - loss: 1.2980 - val_loss: 1.2962\n",
      "Epoch 31/200\n",
      "33500/33500 - 13s - loss: 1.2976 - val_loss: 1.2962\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "33500/33500 - 12s - loss: 1.2985 - val_loss: 1.2962\n",
      "Epoch 33/200\n",
      "33500/33500 - 13s - loss: 1.2990 - val_loss: 1.2962\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "33500/33500 - 12s - loss: 1.2996 - val_loss: 1.2962\n",
      "Epoch 35/200\n",
      "33500/33500 - 12s - loss: 1.3013 - val_loss: 1.2962\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "33500/33500 - 12s - loss: 1.2989 - val_loss: 1.2962\n",
      "Epoch 37/200\n",
      "33500/33500 - 13s - loss: 1.2991 - val_loss: 1.2962\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "33500/33500 - 13s - loss: 1.2981 - val_loss: 1.2962\n",
      "Epoch 39/200\n",
      "33500/33500 - 13s - loss: 1.2998 - val_loss: 1.2962\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "33500/33500 - 12s - loss: 1.3006 - val_loss: 1.2962\n",
      "Epoch 41/200\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.title('Training History')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "predict_val = model.predict(X_val)\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(labels):\n",
    "        df[label] = y_val[:,i]\n",
    "        df[label + '_pred'] = predict_val[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
